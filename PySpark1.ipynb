{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"galileo":{"remoteenvid":162}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bf4ae268-dcb3-4d70-a6b1-442b4b51a0a8","cell_type":"markdown","source":"## 使用Jupyter Notebook运行EMR Spark\n\nSpark是目前最流行的大数据计算引擎，我们在Jupyter Notebook的默认Python运行环境中实现了若干魔术方法，能让你轻松的使用Jupyter运行PySpark\n","metadata":{"tags":[]}},{"id":"490200d1-a56d-4ace-bfe4-8443075b7f54","cell_type":"markdown","source":"#### 在正式运行Spark前，需要加载emrmagic插件","metadata":{}},{"id":"0a3571af-37d6-4f6f-b839-1a36eeb8ad92","cell_type":"code","source":"%load_ext emrmagic","metadata":{"trusted":true,"pycharm":{"name":"#%%\n"},"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:07:38.490382Z","iopub.execute_input":"2023-08-10T08:07:38.490740Z","iopub.status.idle":"2023-08-10T08:07:38.501056Z","shell.execute_reply.started":"2023-08-10T08:07:38.490712Z","shell.execute_reply":"2023-08-10T08:07:38.500369Z"}},"outputs":[{"name":"stdout","text":"The emrmagic extension is already loaded. To reload it, use:\n  %reload_ext emrmagic\n","output_type":"stream"}],"execution_count":1},{"id":"7d4e6001-43b8-4b46-adde-89a5755346d1","cell_type":"markdown","source":"#### emrmagic插件主要包含以下魔术命令\n\n- %%spark.set_config: 用于配置Spark运行时参数，逻辑上你可以配置所有当前EMR Spark版本支持的参数，如executor数量、driver/executor内存、dynamicAllocation等等。\n- %spark.load_config: 用于查看当前Spark参数配置。\n- %spark.get_session: 用于在EMR集群中以yarn client模式启动Spark作业，启动参数使用spark.set_config配置的参数。\n- %%spark.sql: 在启动Spark作业后使用执行SparkSQL，作用相当于`spark.sql('sql statement')`。\n- %spark.stop_session: 用于终止当前Notebook开启的Spark作业。\n\n以下Notebook将举例说明上述魔术命令的使用方法","metadata":{}},{"id":"67d0a8de-084b-4ca6-9997-5ef8b4530bc9","cell_type":"markdown","source":"#### spark.set_config\n\n这是一个cell magic，配置内容采用k=v的模式，在一个Notebook只存在一份Spark配置，你可以在启动Spark作业前的任意时刻改动配置。\n如需查看Spark默认配置，可执行以下的命令","metadata":{}},{"id":"ef86750d-cc2b-4145-a52e-2b57e3f4289a","cell_type":"code","source":"!cat /etc/emr/spark-conf/spark-defaults.conf","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:17:27.895124Z","iopub.execute_input":"2023-08-10T08:17:27.895517Z","iopub.status.idle":"2023-08-10T08:17:28.363368Z","shell.execute_reply.started":"2023-08-10T08:17:27.895487Z","shell.execute_reply":"2023-08-10T08:17:28.362465Z"}},"outputs":[{"name":"stdout","text":"cat: /etc/emr/spark-conf/spark-defaults.conf: No such file or directory\n","output_type":"stream"}],"execution_count":10},{"id":"564d5bc4-6d97-4ec7-81aa-b415e1a4cdbc","cell_type":"code","source":"%%spark.set_config\n\nspark.executor.instances=1\nspark.executor.memory=512m\nspark.executor.cores=2","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:08:00.076172Z","iopub.execute_input":"2023-08-10T08:08:00.076575Z","iopub.status.idle":"2023-08-10T08:08:00.080813Z","shell.execute_reply.started":"2023-08-10T08:08:00.076545Z","shell.execute_reply":"2023-08-10T08:08:00.079669Z"}},"outputs":[],"execution_count":4},{"id":"2582b8cb-5c5d-425c-9d97-cc6509f70062","cell_type":"markdown","source":"需要注意的是，emrmagic内部会将启动Notebook时选择的Python虚拟环境地址传递给spark.yarn.dist.archives参数，并结合spark.pyspark.python参数用于确保Spark作业Python环境的一致性。\n\n如果对于PySpark的Python环境管理机制不甚了解，建议不要修改该参数。","metadata":{}},{"id":"14015cf6-5583-4d17-bb90-5b464d8f4316","cell_type":"markdown","source":"#### spark.load_config\n\n这是一个line magic，作用为获取当前的Spark配置，可在任意时刻查看，在此不再赘述","metadata":{}},{"id":"3d643896-8e37-49c9-baff-07ea7d4772c7","cell_type":"code","source":"%spark.load_config","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Table","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:08:04.185029Z","iopub.execute_input":"2023-08-10T08:08:04.185413Z","iopub.status.idle":"2023-08-10T08:08:04.192384Z","shell.execute_reply.started":"2023-08-10T08:08:04.185382Z","shell.execute_reply":"2023-08-10T08:08:04.191434Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'spark.executor.instances': '1',\n 'spark.executor.memory': '512m',\n 'spark.executor.cores': '2'}"},"metadata":{}}],"execution_count":5},{"id":"79dbe189-b8ea-49e7-9961-0169dfe961d6","cell_type":"markdown","source":"#### spark.get_session\n\n这是一个line magic，该命令接收一个行参数，用于指定Spark作业名称；返回一个SparkSession和SparkContext对象，用于后续执行Spark程序。每个Notebook只能启动一个Spark作业，重复执行该命令会获取已生成的SparkSession。另外，该SparkSession默认开启Hive支持（即enableHiveSupport），连接EMR集群中的Hive Metastore。","metadata":{}},{"id":"a38185e3-a056-4167-a69d-4143818593ec","cell_type":"code","source":"spark, sc = %spark.get_session test","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:08:07.777651Z","iopub.execute_input":"2023-08-10T08:08:07.778035Z","iopub.status.idle":"2023-08-10T08:08:22.639925Z","shell.execute_reply.started":"2023-08-10T08:08:07.778005Z","shell.execute_reply":"2023-08-10T08:08:22.638866Z"}},"outputs":[{"name":"stderr","text":"ERROR StatusLogger Reconfiguration failed: No configuration found for '452b3a41' at 'null' in 'null'\nERROR StatusLogger Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"23/08/10 16:08:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n23/08/10 16:08:11 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n23/08/10 16:08:12 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n","output_type":"stream"}],"execution_count":6},{"id":"81631da5-2853-44d0-b508-15d44a1b1ce0","cell_type":"code","source":"spark","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Table","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:08:24.568134Z","iopub.execute_input":"2023-08-10T08:08:24.568533Z","iopub.status.idle":"2023-08-10T08:08:25.425851Z","shell.execute_reply.started":"2023-08-10T08:08:24.568502Z","shell.execute_reply":"2023-08-10T08:08:25.425028Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7f33e182ac50>","text/html":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.224.143.173:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>test</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}],"execution_count":7},{"id":"a06f8c08-9a43-43f9-a39a-afc021e7a3d9","cell_type":"markdown","source":"接下来即可开始编写PySpark程序\n","metadata":{}},{"id":"fb9e0b47-810e-495e-b262-8429091ca3c3","cell_type":"code","source":"sum = sc.range(1,10).sum()\nprint(\"Sum = \" + str(sum))\n\ndf = spark.read.json(\"file:///opt/apps/SPARK3/spark-current/examples/src/main/resources/people.json\")\ndf.printSchema()\n\ndf.show()","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:12:57.988750Z","iopub.execute_input":"2023-08-10T08:12:57.989132Z","iopub.status.idle":"2023-08-10T08:13:02.351277Z","shell.execute_reply.started":"2023-08-10T08:12:57.989103Z","shell.execute_reply":"2023-08-10T08:13:02.350381Z"}},"outputs":[{"name":"stdout","text":"Sum = 45\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n","output_type":"stream"}],"execution_count":9},{"id":"5a56df53-118a-4f50-93e1-05f5b3b64263","cell_type":"markdown","source":"可以直接通过EMR Spark访问oss中的数据","metadata":{}},{"id":"6416aad9-abfd-4c34-bf09-9406510409b2","cell_type":"code","source":"import os\n\nbucket = os.environ['VENV'].split('/')[2]\nusername = os.environ['USER']\n\nspark.read.json('oss://{bucket}/jupyter/notebook/{username}/tutorial'.format(bucket=bucket, username=username)).show()","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false}},"outputs":[{"name":"stdout","text":"+--------------------+--------------------+--------+--------------+\n|               cells|            metadata|nbformat|nbformat_minor|\n+--------------------+--------------------+--------+--------------+\n|[{markdown, null,...|{{Spark - Python ...|       4|             5|\n|[{markdown, null,...|{{Spark - Python ...|       4|             5|\n|[{markdown, null,...|{{Spark - Python ...|       4|             5|\n|[{code, 1, c254df...|{{Spark - Python ...|       4|             5|\n+--------------------+--------------------+--------+--------------+\n\n","output_type":"stream"}],"execution_count":8},{"id":"a591dcad-eb56-43eb-a812-f781a6759415","cell_type":"markdown","source":"使用Spark mllib运行简单的机器学习程序","metadata":{}},{"id":"3ca6746c-796e-4b0d-bcc1-29aeed0efd5f","cell_type":"code","source":"!pip install pandas numpy -i http://mirrors.cloud.aliyuncs.com/pypi/simple --trusted-host mirrors.cloud.aliyuncs.com","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:21:47.974161Z","iopub.execute_input":"2023-08-10T08:21:47.974557Z","iopub.status.idle":"2023-08-10T08:21:57.007238Z","shell.execute_reply.started":"2023-08-10T08:21:47.974526Z","shell.execute_reply":"2023-08-10T08:21:57.006377Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: http://mirrors.cloud.aliyuncs.com/pypi/simple\nCollecting pandas\n  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/e3/59/35a2892bf09ded9c1bf3804461efe772836a5261ef5dfb4e264ce813ff99/pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting numpy\n  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/71/3c/3b1981c6a1986adc9ee7db760c0c34ea5b14ac3da9ecfcf1ea2a4ec6c398/numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /etc/dsw/lib/python3.10/site-packages (from pandas) (2.8.2)\nCollecting pytz>=2020.1 (from pandas)\n  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/7f/99/ad6bd37e748257dd70d6f85d916cafe79c0b0f5e2e95b11f7fbc82bf3110/pytz-2023.3-py2.py3-none-any.whl (502 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tzdata>=2022.1 (from pandas)\n  Downloading http://mirrors.cloud.aliyuncs.com/pypi/packages/d5/fb/a79efcab32b8a1f1ddca7f35109a50e4a80d42ac1c9187ab46522b2407d7/tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.5 in /etc/dsw/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nInstalling collected packages: pytz, tzdata, numpy, pandas\nSuccessfully installed numpy-1.25.2 pandas-2.0.3 pytz-2023.3 tzdata-2023.3\n","output_type":"stream"}],"execution_count":15},{"id":"65898964-9e5c-48ca-8cdf-530fd1545c5c","cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\n\n# Load training data\ntraining = spark.read.format(\"libsvm\").load(\"file:///opt/apps/SPARK3/spark-current/data/mllib/sample_libsvm_data.txt\")\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the model\nlrModel = lr.fit(training)\n\n# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(lrModel.coefficients))\nprint(\"Intercept: \" + str(lrModel.intercept))\n\n# We can also use the multinomial family for binary classification\nmlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n\n# Fit the model\nmlrModel = mlr.fit(training)\n\n# Print the coefficients and intercepts for logistic regression with multinomial family\nprint(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\nprint(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:24:03.268766Z","iopub.execute_input":"2023-08-10T08:24:03.269168Z","iopub.status.idle":"2023-08-10T08:24:10.926764Z","shell.execute_reply.started":"2023-08-10T08:24:03.269133Z","shell.execute_reply":"2023-08-10T08:24:10.925754Z"}},"outputs":[{"name":"stdout","text":"23/08/10 16:24:03 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"23/08/10 16:24:06 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n23/08/10 16:24:06 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n23/08/10 16:24:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n23/08/10 16:24:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\nCoefficients: (692,[272,300,323,350,351,378,379,405,406,407,428,433,434,435,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.520689871384157e-05,-8.11577314684704e-05,3.814692771846389e-05,0.0003776490540424341,0.0003405148366194407,0.0005514455157343111,0.00040853861160969167,0.00041974673327494573,0.0008119171358670032,0.0005027708372668752,-2.392926040660149e-05,0.0005745048020902299,0.000903754642680371,7.818229700243959e-05,-2.17875519529124e-05,-3.402165821789581e-05,0.0004966517360637634,0.0008190557828370371,-8.017982139522661e-05,-2.743169403783574e-05,0.00048108322262389896,0.00048408017626778744,-8.926472920010679e-06,-0.0003414881233042728,-8.950592574121448e-05,0.0004864546911689218,-8.478698005186158e-05,-0.0004234783215831764,-7.296535777631296e-05])\nIntercept: -0.5991460286401442\nMultinomial coefficients: 2 X 692 CSRMatrix\n(0,272) 0.0001\n(0,300) 0.0001\n(0,350) -0.0002\n(0,351) -0.0001\n(0,378) -0.0003\n(0,379) -0.0002\n(0,405) -0.0002\n(0,406) -0.0004\n(0,407) -0.0002\n(0,433) -0.0003\n(0,434) -0.0005\n(0,435) -0.0001\n(0,456) 0.0\n(0,461) -0.0002\n(0,462) -0.0004\n(0,483) 0.0001\n..\n..\nMultinomial intercepts: [0.2750587585718083,-0.2750587585718083]\n","output_type":"stream"}],"execution_count":16},{"id":"cf829205-f954-4185-8620-20b93e01d9cc","cell_type":"markdown","source":"#### spark.sql\n\n该命令既是line magic又是cell magic，下面通过具体的例子来介绍改magic的主要特性。","metadata":{}},{"id":"9f84a8be-c9bd-4646-b189-2c7a381950ac","cell_type":"markdown","source":"在cell中编写单句SQL执行","metadata":{}},{"id":"3e48d4c2-55b3-4413-a88e-a9da12638b2a","cell_type":"code","source":"%%spark.sql\n\nshow tables","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false},"execution":{"iopub.status.busy":"2023-08-10T08:24:20.047219Z","iopub.execute_input":"2023-08-10T08:24:20.047628Z","iopub.status.idle":"2023-08-10T08:24:24.829781Z","shell.execute_reply.started":"2023-08-10T08:24:20.047598Z","shell.execute_reply":"2023-08-10T08:24:24.828056Z"}},"outputs":[{"name":"stdout","text":"2023-08-10 16:24:20,228 Thread-4 ERROR Reconfiguration failed: No configuration found for '22c6bb19' at 'null' in 'null'\n16:24:24.050 [Thread-4] ERROR com.aliyun.datalake.metastore.common.STSHelper - can't get ststoken afer retry:3 times, due to Failed to connect to /100.100.100.200:80\njava.net.ConnectException: Failed to connect to /100.100.100.200:80\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:249) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.RealConnection.connect(RealConnection.java:167) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:258) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:127) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:257) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.RealCall.execute(RealCall.java:93) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.retryGetResponse(STSHelper.java:112) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.retryGetResponse(STSHelper.java:121) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.retryGetResponse(STSHelper.java:121) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.getResponse(STSHelper.java:100) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.getEMRSTSTokenNew(STSHelper.java:142) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.getEMRSTSToken(STSHelper.java:82) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.getLatestSTSToken(STSHelper.java:70) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.DefaultDataLakeMetaStore.reInitDataLakeClient(DefaultDataLakeMetaStore.java:155) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.DefaultDataLakeMetaStore.<init>(DefaultDataLakeMetaStore.java:82) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.hive.common.MetastoreFactory.getMetaStore(MetastoreFactory.java:56) ~[metastore-client-hive-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.hive2.DlfMetaStoreClientDelegate.<init>(DlfMetaStoreClientDelegate.java:98) ~[metastore-client-hive2-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.hive2.DlfMetaStoreClient.<init>(DlfMetaStoreClient.java:71) ~[metastore-client-hive2-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.hive2.DlfSessionMetaStoreClient.<init>(DlfSessionMetaStoreClient.java:32) ~[metastore-client-hive2-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.hive2.ProxyMetaStoreClient.initDlfClient(ProxyMetaStoreClient.java:183) ~[metastore-client-hive2-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.hive2.ProxyMetaStoreClient.lambda$new$0(ProxyMetaStoreClient.java:79) ~[metastore-client-hive2-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.hive2.ProxyMetaStoreClient.createClient(ProxyMetaStoreClient.java:94) ~[metastore-client-hive2-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.hive2.ProxyMetaStoreClient.<init>(ProxyMetaStoreClient.java:79) ~[metastore-client-hive2-0.2.21.jar:?]\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_352]\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_352]\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_352]\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_352]\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740) ~[hive-metastore-2.3.9.jar:2.3.9]\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[hive-metastore-2.3.9.jar:2.3.9]\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[hive-metastore-2.3.9.jar:2.3.9]\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[hive-metastore-2.3.9.jar:2.3.9]\n\tat com.aliyun.datalake.metastore.hive2.DlfMetaStoreClientFactory.createMetaStoreClient(DlfMetaStoreClientFactory.java:46) ~[metastore-client-hive2-0.2.21.jar:?]\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3744) ~[hive-exec-2.3.9-core.jar:2.3.9]\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3794) ~[hive-exec-2.3.9-core.jar:2.3.9]\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3774) ~[hive-exec-2.3.9-core.jar:2.3.9]\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1588) ~[hive-exec-2.3.9-core.jar:2.3.9]\n\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1577) ~[hive-exec-2.3.9-core.jar:2.3.9]\n\tat org.apache.spark.sql.hive.client.Shim_v0_12.databaseExists(HiveShim.scala:609) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:398) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23) ~[scala-library-2.12.15.jar:?]\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:298) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:229) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:228) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:278) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:398) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:223) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23) ~[scala-library-2.12.15.jar:?]\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:223) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:170) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:168) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:70) ~[spark-hive_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:122) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:122) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1050) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1036) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1028) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.listTables(V2SessionCatalog.scala:57) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560) ~[spark-catalyst_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]\n\tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) ~[py4j-0.10.9.5.jar:?]\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) ~[py4j-0.10.9.5.jar:?]\n\tat py4j.Gateway.invoke(Gateway.java:282) ~[py4j-0.10.9.5.jar:?]\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) ~[py4j-0.10.9.5.jar:?]\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79) ~[py4j-0.10.9.5.jar:?]\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182) ~[py4j-0.10.9.5.jar:?]\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106) ~[py4j-0.10.9.5.jar:?]\n\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_352]\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_352]\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_352]\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_352]\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_352]\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_352]\n\tat java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_352]\n\tat com.aliyun.datalake.external.okhttp3.internal.platform.Platform.connectSocket(Platform.java:129) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:247) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\t... 114 more\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_342/190475453.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.sql'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nshow tables\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/apps/galileo_kernel/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/apps/galileo_kernel/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/apps/galileo_kernel/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/apps/galileo_kernel/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/apps/galileo_kernel/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/apps/galileo_kernel/lib/python3.7/site-packages/emrmagic/spark_magic.py\u001b[0m in \u001b[0;36mexecute_sql\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/apps/galileo_kernel/lib/python3.7/site-packages/emrmagic/spark_magic.py\u001b[0m in \u001b[0;36mexecute_sql\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msql\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/apps/SPARK3/spark3-current/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/apps/SPARK3/spark3-current/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/apps/SPARK3/spark3-current/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate com.aliyun.datalake.metastore.hive2.ProxyMetaStoreClient"],"ename":"AnalysisException","evalue":"org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate com.aliyun.datalake.metastore.hive2.ProxyMetaStoreClient","output_type":"error"},{"name":"stdout","text":"16:24:27.186 [dlf-token-refresher-0] ERROR com.aliyun.datalake.metastore.common.STSHelper - can't get ststoken afer retry:3 times, due to Failed to connect to /100.100.100.200:80\njava.net.ConnectException: Failed to connect to /100.100.100.200:80\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:249) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.RealConnection.connect(RealConnection.java:167) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:258) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:127) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:257) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.RealCall.execute(RealCall.java:93) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.retryGetResponse(STSHelper.java:112) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.retryGetResponse(STSHelper.java:121) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.retryGetResponse(STSHelper.java:121) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.getResponse(STSHelper.java:100) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.getEMRSTSTokenNew(STSHelper.java:142) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.getEMRSTSToken(STSHelper.java:82) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.getLatestSTSToken(STSHelper.java:70) ~[metastore-client-common-0.2.21.jar:?]\n\tat com.aliyun.datalake.metastore.common.STSHelper.lambda$initSTSHelper$0(STSHelper.java:52) ~[metastore-client-common-0.2.21.jar:?]\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_352]\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) ~[?:1.8.0_352]\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_352]\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) ~[?:1.8.0_352]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_352]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_352]\n\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_352]\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\n\tat java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_352]\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_352]\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_352]\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_352]\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_352]\n\tat java.net.Socket.connect(Socket.java:607) ~[?:1.8.0_352]\n\tat com.aliyun.datalake.external.okhttp3.internal.platform.Platform.connectSocket(Platform.java:129) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\tat com.aliyun.datalake.external.okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:247) ~[aliyun-java-sdk-dlf-shaded-0.2.9.jar:?]\n\t... 32 more\n","output_type":"stream"}],"execution_count":17},{"id":"84d61415-fc26-4744-aa59-5595ade99aed","cell_type":"markdown","source":"支持编写多句SQL，SQL之间使用分号分隔，展示最后一条语句的返回结果","metadata":{}},{"id":"3ef7edaf-7731-4c3b-bdd0-bfc9f3f4abb0","cell_type":"code","source":"%%spark.sql\n\nuse default;\ndrop table if exists test_table;\ncreate table test_table (a string, b int);\ninsert into test_table values(\"abc\", 1), (\"def\", 2);\nselect a, sum(b) from test_table group by a","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"     a  sum(b)\n0  abc       1\n1  def       2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>sum(b)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abc</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>def</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"id":"83aaafdb-e739-48fa-b571-a8656c7a4808","cell_type":"code","source":"x = 'abc'","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false}},"outputs":[],"execution_count":18},{"id":"88755544-4cde-4705-a9a5-0f88fd18e344","cell_type":"markdown","source":"支持向SQL语句中传入变量","metadata":{}},{"id":"0f30a2db-9a24-4ba2-ba1c-734668249dcb","cell_type":"code","source":"%%spark.sql\nselect * from test_table where a = '{x}'","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"     a  b\n0  abc  1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abc</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"id":"8e131ead-072f-4d79-a784-ef2a6074b2e6","cell_type":"markdown","source":"支持将最后一句SQL的查询结果传递给一个变量（使用`-o`参数），变量类型为Spark DataFrame","metadata":{}},{"id":"01730585-910f-4225-9f02-cae1a1df0a03","cell_type":"code","source":"%%spark.sql -o df\n\nselect a, sum(b) from test_table group by a","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"     a  sum(b)\n0  abc       1\n1  def       2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>sum(b)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abc</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>def</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"id":"4af4cc6f-9688-4e54-87b4-80593effbc4e","cell_type":"code","source":"df.show()","metadata":{"trusted":true,"galileo":{"version":"1.0.0","sql_output_active_tab":"Log","collapsed_input":false,"collapsed_output":false}},"outputs":[{"name":"stdout","text":"+---+------+\n|  a|sum(b)|\n+---+------+\n|abc|     1|\n|def|     2|\n+---+------+\n\n","output_type":"stream"}],"execution_count":21}]}